
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Pranav Guru</title>
    <link rel="stylesheet" href="styles.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
</head>
<body>
    <div class="container">
        <header>
            <div class="header-top">
                <h1>Pranav Guruprasad</h1>
                <button class="theme-toggle" id="theme-toggle" aria-label="Toggle theme">
                    <i class="fas fa-sun"></i>
                </button>
            </div>
            <p>I am a researcher and engineer interested in the intersection of multimodal understanding and action taking capabilities of AI systems.</p>
        </header>
        <main>
            <p>I am currently building <a href="https://metarch.ai">metarch</a> to build a new class of foundation models and AI systems that can take actions over long horizons in the software world. I studied computer science, specifically machine learning at Georgia Tech and BITS Goa. I made early contributions to <a href="https://www.app.got-it.ai/">got-it-ai</a>, the world's first fully autonomous conversational AI platform. I briefly worked in defense tech on multimodal systems to safeguard personnel. I have also worked on translation at IBM Watson ML.</p>
            
            <section id="publications">
                <h2>Selected Publications</h2>
                <ul>
                    <li><a href="https://scholar.google.com/citations?view_op=view_citation&hl=en&user=vrtIqhEAAAAJ&citation_for_view=vrtIqhEAAAAJ:zYLM7Y9cAGgC">An Open-Source Software Toolkit & Benchmark Suite for the Evaluation and Adaptation of Multimodal Action Models</a></li>
                    <li><a href="https://scholar.google.com/citations?view_op=view_citation&hl=en&user=vrtIqhEAAAAJ&citation_for_view=vrtIqhEAAAAJ:qjMakFHDy7sC">Benchmarking Vision, Language, & Action Models in Procedurally Generated, Open Ended Action Environments</a></li>
                    <li><a href="https://scholar.google.com/citations?view_op=view_citation&hl=en&user=vrtIqhEAAAAJ&citation_for_view=vrtIqhEAAAAJ:IjCSPb-OGe4C">KULCQ: An Unsupervised Keyword-based Utterance Level Clustering Quality Metric</a></li>
                    <li><a href="https://scholar.google.com/citations?view_op=view_citation&hl=en&user=vrtIqhEAAAAJ&citation_for_view=vrtIqhEAAAAJ:2osOgNQ5qMEC">Jill watson: A virtual teaching assistant powered by chatgpt</a></li>
                    <li><a href="https://scholar.google.com/citations?view_op=view_citation&hl=en&user=vrtIqhEAAAAJ&citation_for_view=vrtIqhEAAAAJ:d1gkVwhDpl0C">An end-to-end, interactive deep learning based annotation system for cursive and print English handwritten text</a></li>
                </ul>
            </section>

            <div class="social-links">
                <a href="https://x.com/pranavguru13" title="Twitter"><i class="fab fa-twitter"></i></a>
                <a href="https://scholar.google.com/citations?user=vrtIqhEAAAAJ&hl=en" title="Google Scholar"><i class="fas fa-graduation-cap"></i></a>
            </div>
        </main>
    </div>
    <script src="js/script.js"></script>
</body>
</html>
